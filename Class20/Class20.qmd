---
title: "More Hierarchical Models in Stan"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---

## Administrative Issues

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```

-   HW2 is graded in Canvas
-   HW3 will be posted on Canvas tonight and due before class Monday
-   Syllabus for next week is updated on Canvas to reflect:

    -   Monday: Causal Models
    -   Tuedsay: Review
    -   Wednesday: Reading day
    -   Thursday: Other classes are having exams
    -   Friday: Exam for 5224

## Data for a Binomial GLM

```{r, message = FALSE}
library(dplyr)
funding <- 
  tibble(
    discipline   = rep(c("Chemical sciences", "Physical sciences", 
                         "Physics", 
                         "Humanities", "Technical sciences",  
                         "Interdisciplinary", "Earth/life sciences", 
                         "Social sciences", "Medical sciences"),
                     each = 2) %>% as.factor,
    female       = rep(0:1, times = 9),
    applications = c(83, 39, 135, 39, 67, 9, 230, 166, 189, 
                     62, 105, 78, 156, 126, 425, 409, 245, 260),
    awards       = c(22, 10, 26, 9, 18, 2, 33, 32, 30, 
                     13, 12, 17, 38, 18, 65, 47, 46, 29)
  )
```

## Calling Stan

```{r}
library(cmdstanr)
```

```{r}
#| label: grants2
#| cache: true
#| output: false
mod <- cmdstan_model("grants2.stan")
dat <- with(funding, 
            list(N = nrow(funding), J = nlevels(discipline),
                 discipline = as.integer(discipline), female = female,
                 applications = applications, awards = awards,
                 prior_only = 0, 
                 m = c(qlogis(0.25), 0), s = c(.2, .1), r = c(2, 2))
            )
post <- mod$sample(dat, output_dir = getwd())
```

## Warnings

```{r}
post$cmdstan_diagnose()
```

## Option 1: Increasing `adept_delta`

```{r}
#| output: false
post <- mod$sample(dat, seed = 1234, output_dir = getwd(), 
                   adapt_delta = 0.9999, max_treedepth = 11)
```
```{r}
post$cmdstan_diagnose()
```

## Numerical Summary

```{r}
post$summary() |> filter(variable != "lp__") |> select(-rhat) # rhat was ok
```

## Multivariate Normal Distribution

-   Let $X = \begin{bmatrix} X_1 & \cdots & X_K\end{bmatrix}^\top$ where $X_k$ is i.i.d. std. normal

-   Let $\boldsymbol{\mu}$ be a fixed vector of size $K$ and $R$ be a fixed upper-triangular $K \times K$ matrix with positive diagonal elements

-   If $Y = \boldsymbol{\mu} + R^\top X$, then $X = R^{-\top}\left(Y - \boldsymbol{\mu}\right)$ and all the partial derivatives of $X$ wrt $Y$ are in $J = R^{-\top}$, so $$\begin{eqnarray} f_Y\left(\mathbf{y} \mid \boldsymbol{\mu}, R\right) &=&
    \frac{e^{-\frac{1}{2} \left(\mathbf{y} - \boldsymbol{\mu}\right)^\top R^{-1}R^{-\top}\left(\mathbf{y} - \boldsymbol{\mu}\right)}}{\left(2\pi\right)^{\frac{K}{2}}} \times \left|\det R^{-\top}\right| \\ 
    f_Y\left(\mathbf{y} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}\right) &=& 
    \frac{e^{-\frac{1}{2} \left(\mathbf{y} - \boldsymbol{\mu}\right)^\top \boldsymbol{\Sigma}^{-1} \left(\mathbf{y} - \boldsymbol{\mu}\right)}}{\left(2\pi\right)^{\frac{K}{2}}\det\left(\boldsymbol{\Sigma}\right)^{\frac{1}{2}}} \text{with } \boldsymbol{\Sigma} = R^\top R\end{eqnarray}$$

## Properties of Multivariate Normal RVs {.smaller}

-   If $K = 2$, it simplifies to bivariate normal. If $K = 1$, it simplifies to univariate normal.

-   $\mathbb{E}\left[Y\right] = \mathbb{E}\left[\boldsymbol{\mu} + R^\top X\right] = \boldsymbol{\mu} + R^\top \mathbb{E}\left[X\right] = \boldsymbol{\mu}$ and the variance-covariance matrix is $\mathbb{E}\left[\left(Y - \boldsymbol{\mu}\right) \left(Y - \boldsymbol{\mu}\right)^\top \right] = \mathbb{E}\left[R^\top X X^\top R\right] = R^\top \mathbb{E}\left[X X^\top\right] R = R^\top I R = \boldsymbol{\Sigma}$

-   The order of the $K$ variables is basically irrelevant and can be permuted

-   If $K_1 \leq K$ and $A$ and $B$ index a subsets of size $K_1$ and $K - K_1$ respectively so that $\boldsymbol{\mu} = \begin{bmatrix} \boldsymbol{\mu}_A \\ \boldsymbol{\mu}_B \end{bmatrix}$ and $\boldsymbol{\Sigma} = \begin{bmatrix} \boldsymbol{\Sigma}_{AA} & \boldsymbol{\Sigma}_{AB} \\ \boldsymbol{\Sigma}_{AB}^\top & \boldsymbol{\Sigma}_{BB} \end{bmatrix}$, then

    -   $\mathbf{y}_A = \begin{bmatrix} y_1 & \dots y_{K_1} \end{bmatrix}^\top$ is marginally $K_1$-variate normal with expectation $\boldsymbol{\mu}_A$ and variance-covariance matrix $\boldsymbol{\Sigma}_{AA}$; i.e. you can ignore the other $K - K_1$ variables

    -   $\mathbf{y}_A \mid \mathbf{y}_B$ is conditionally $K_1$-variate normal with expectation vector $\boldsymbol{\mu}_A + \boldsymbol{\Sigma}_{AB}^\top \boldsymbol{\Sigma}_{BB}^{-1} \left(\mathbf{y}_B - \boldsymbol{\mu}_B\right)$ and a positive semi-definite variance-covariance matrix that involves a Schur complement $\boldsymbol{\Sigma}_{AA} - \boldsymbol{\Sigma}_{AB}^\top \boldsymbol{\Sigma}_{BB}^{-1} \boldsymbol{\Sigma}_{AB}$

    -   $\mathbf{y}_A$ and $\mathbf{y}_B$ are independent iff $\boldsymbol{\Sigma}_{AB} = \mathbf{0}$

-   The $K$-variate normal distribution is the easiest $K$-variate continuous distribution

## Option 2: Reparameterize

```{stan, output.var = "reparam", eval = FALSE}
parameters {
  // the same except for:
  vector[J] a_std; // deviations in intercept in z-scores
  vector[J] b_std; // deviations in slope in z-scores
}
transformed parameters {
  vector[J] a = 0 + sigma[1] * a_std;
  vector[J] b = 0 + sigma[2] / sigma[1] * rho * (a - 0)
              + sigma[2] * sqrt(1 - square(rho)) * b_std;
}
model {
  // the same except for:
  target += std_normal_lpdf(a_std); // implies a is normal
  target += std_normal_lpdf(b_std); // implies b | a is normal 
}
```

## Reestimate with Reparameterization

```{r}
#| label: grants3
#| cache: true
mod_reparam <- cmdstan_model("grants3.stan") 
```
```{r}
#| output: false
post_reparam <- mod_reparam$sample(dat, seed = 1234, output_dir = getwd())
```
```{r}
post_reparam$summary() |> filter(variable != "lp__") |> select(-rhat)
```

## PSISLOOCV Estimator of the ELPD

```{r}
post_reparam$loo()
```

## Return of LKJ

-   Often the covariance matrix to a multivariate normal is unknown
-   $\boldsymbol{\Sigma} = \boldsymbol{\Delta} \mathbf{L} \mathbf{L}^\top \boldsymbol{\Delta}$ where $\boldsymbol{\Delta}$ is a diagonal matrix of standard deviations and $\mathbf{L} \mathbf{L}^\top$ is a correlation matrix, which can be given an LKJ prior
-   It is actually more efficient to put a LKJ-like prior on $\mathbf{L}$ that implies $\mathbf{L} \mathbf{L}^\top$ is distributed LKJ

## Frequentist Example

```{r, message = FALSE, warning = FALSE}
poll <- readRDS("GooglePoll.rds") # WantToWin is coded as 1 for Romney and 0 for Obama
poll$Income[poll$Income == "150,000+"] <- "100,000-149,999"
library(dplyr)
collapsed <- filter(poll, !is.na(WantToWin)) %>%
             group_by(Region, Gender, Urban_Density, Age, Income) %>%
             summarize(Romney = sum(grepl("Romney", WantToWin)), 
                       Obama = n() - Romney) %>%
             na.omit
```
```{r, glmer, cache = TRUE, results = "hide", warning = FALSE}
mle <- lme4::glmer(cbind(Romney, Obama) ~ 
                     Gender + Urban_Density + Age + Income +
                    (Gender + Urban_Density + Age + Income | Region),
                   data = collapsed, family = binomial(link = "logit"))
```

> - For models that are more complicated than `(1 + x | g)`, the MLE of $\boldsymbol{\Sigma}$ 
  usually implies that $\widehat{\boldsymbol{\Sigma}}^{-1}$ does not exist.

## Frequent Frequentist Problem

```{r}
summary(mle)
```
