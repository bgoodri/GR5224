---
title: "Model Checking and Comparison"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---

## Introduction

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```
-   People often feel that different quantitative methods should be used in different situations:
    -   If you have a RCT, use design-based inference
    -   If you have a survey, use MLE
    -   If you want to predict, use supervised learning
    -   If you have strong priors, use Bayesian
-   But Bayesian inference is a belief management system and should be applicable to all of those scenarios and more
-   Bayesians can generate predictions, so we want to contrast that with the predictions of supervised learning

## What Is Supervised Learning?

-   Supervised Learning is like Frequentism without probability

-   Main goal is to predict future outcomes, rather than interpret

    -   Usually adds a penalty term to the log-likelihood
    -   Can use more flexible forms than linearity
    -   Requires splitting into $\approx 80$% training and $\approx 20$% testing
    -   Usually subsplits the training data into $K$ folds (or bootstraps) to choose tuning parameters for the penalty

-   Bayes Rule sometimes is referred to, but it is not Bayesian

-   Maximizes a function in the training data to produce a point estimate of $\boldsymbol{\theta}$ that is then used to predict in the testing data

## Penalization vs. Priors

-   These penalty functions are often some [prior](https://osf.io/4ev8h/) log-kernel conditional on an unknown tuning parameter, $\lambda$ ![van Erp, Oberski, and Mulder (2019)](table1.png)

-   Penalty functions make for [poor priors](https://statmodeling.stat.columbia.edu/2017/11/02/king-must-die/) because they are intended to shift the mode rather than reflect beliefs

## Generative Model with Laplace Priors

::: columns
::: {.column width="56%"}
Math Notation $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_n \\ \forall n: \epsilon_n & \thicksim & \mathcal{N}\left(0,\sigma\right) \\ \forall n: \eta_n & \equiv & \mu + \sum_{k = 1}^K \beta_k \left(x_{nk} - \overline{x}_k\right) \\ \mu & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \forall k: \beta_k & \equiv & -\lambda / \sigma \ \mathrm{sign}\left(\theta_k\right) \ln\left(1 - \left|\theta_k\right|\right) \\ \lambda & \thicksim & ??? \\ \sigma & \thicksim & \mathcal{E}\left(r\right) \\ \forall k: \theta_k & \thicksim & \mathcal{U}\left(-1,1\right) \end{eqnarray*}$
:::

::: {.column width="44%"}
::: fragment
Code to Draw Parameters

```{r}
library(dplyr)
R <- 10000
prior <- tibble(
  theta = runif(R, -1, 1),
  sigma = rexp(R, 1),
  # a choice for ???
  lambda = rexp(R, 1 / .75),
  beta = -lambda / sigma * 
    sign(theta) *
    log(1 - abs(theta)))
```

\
But supervised learning takes $\lambda$ to be a point, rather than a random variate from a prior
:::
:::
:::

## Plotting Prior Draws of $\beta \bigcap \bcancel{\lambda} \bigcap \bcancel{\sigma}$

```{r}
library(ggplot2); ggplot(prior) + geom_density(aes(beta)) + xlim(-4,4)
```

## Choosing Tuning Parameters

-   How does supervised learning choose $\lambda$?

    -   Split data into $N_1$ training observations and $N_2$ testing observations with $N_1 / N_2 \approx 4$

    -   Split training data into $K$ folds each with $N_1 / K$ rows

        -   Guess $\lambda$, solve for $\boldsymbol{\beta}$ using $K - 1$ folds, predict outcomes in $K$-th fold, average loss over observations

        -   Improve guess for $\lambda$, stop when average loss is stable

-   Predict testing outcomes given $\widehat{\boldsymbol{\beta}}$ and average loss over $N_2$ observations. The loss function can be the log-likelihood but is often [improper](https://en.wikipedia.org/wiki/Scoring_rule#Propriety_and_consistency).

## Loss Functions

-   Mean-squared error (in testing): $\frac{1}{N_2} \sum_{n = 1}^{N_2} \left(y_n - \eta_n\right)^2$

-   MSE is proportional to a Gaussian log-density (given $\sigma$): $-N_2\log\left(\sigma \sqrt{2\pi}\right) - \frac{1}{2 \sigma^2}\sum_{n = 1}^{N_2}\left(y_n - \eta_n\right)^2$

-   You will often see the square root of MSE but a monotonic transformation does not change the ranking of models

-   With binary outcomes, the loss function is usually some function of "correct" and "incorrect" classifications where an observation is classified as successful if $\eta_n > 0$

. . .

-   For Bayesians, a default utility is log density / mass of $Y$

## Expected Log Predictive Density {.smaller}

-   Decision theory says to choose the model that maximizes *expected* utility $$\mbox{ELPD} = \mathbb{E}_Y \ln f\left(y_{N + 1}, y_{N + 2}, \dots, y_{2N} \mid y_1, y_2, \dots, y_N\right) = \\
    \sum_{n = 1}^{N} \int_\Omega 
    \ln f\left(y_{N + n} \mid \mathbf{y}\right) f\left(y_{N + n} \mid \mathbf{y} \right) dy_{N + n} = \\
    \sum_{n = 1}^N \int_\Omega \ln \int_\Theta f\left(y_{N + n} \mid \boldsymbol{\theta}\right) f\left(\boldsymbol{\theta} \mid \mathbf{y}\right) d\boldsymbol{\theta} dy_{N + n} \approx  \\
    \sum_{n = 1}^N \ln f\left(y_n \mid \mathbf{y}_{-n}\right) = \sum_{n = 1}^N
    \ln \int_\Theta f\left(y_n \mid \boldsymbol{\theta}\right) 
    f\left(\boldsymbol{\theta} \mid \mathbf{y}_{-n}\right) d\boldsymbol{\theta}$$

where $y_{-n}$ indicates all but the $n$-th observation (like in R)

. . .

-   $f\left(y_n \mid \boldsymbol{\theta}\right)$ is just the $n$-th likelihood contribution, but can we somehow obtain $f\left(\boldsymbol{\theta} \mid \mathbf{y}_{-n}\right)$ from (draws from) $f\left(\boldsymbol{\theta} \mid \mathbf{y}\right)$? Yes, assuming $y_n$ does not have an outsized influence on the posterior.

## 

> \[T\]he joint probability is the measure we want. Why? $\dots$ It's the unique measure that correctly counts up the relative number of ways each event $\dots$ could happen. \dots \[C\]onsider what happens when we maximize average probability or joint probability. The true data-generating model will not have the highest hit rate. You saw this already with the weatherperson: Assigning zero probability to rain improves hit rate, but it is clearly wrong. In contrast, the true model will have the highest joint probability. \[Y\]ou will sometimes see this measure of accuracy called the log scoring rule, because typically we \[report\] the logarithm of the joint probability $\dots$ If you see an analysis using something else, it is a special case of the log scoring rule or it is a mistake.
