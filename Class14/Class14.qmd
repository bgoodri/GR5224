---
title: "More Model Checking and Comparison"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---

## Advanced GDP Estimate

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```
-   The (initial) estimate of GDP in the second quarter of 2025 will be [released](https://www.bea.gov/) at 8:30AM tomorrow

-   What are your prior beliefs about $\mu$? Should you utilize Okun's Law?

-   How would you proceed differently with $\mu \mid y$ than vs. $\mu \equiv y$?

## 

> \[T\]he joint probability is the measure we want. Why? $\dots$ It's the unique measure that correctly counts up the relative number of ways each event $\dots$ could happen. \dots \[C\]onsider what happens when we maximize average probability or joint probability. The true data-generating model will not have the highest hit rate. You saw this already with the weatherperson: Assigning zero probability to rain improves hit rate, but it is clearly wrong. In contrast, the true model will have the highest joint probability. \[Y\]ou will sometimes see this measure of accuracy called the log scoring rule, because typically we \[report\] the logarithm of the joint probability $\dots$ If you see an analysis using something else, it is a special case of the log scoring rule or it is a mistake.

## Choice in Model Choice {.incremental}

::: incremental
-   Bayesians: Choose the model that maximizes this $\mbox{ELPD} = \mathbb{E}_Y \ln f\left(y_{N + 1}, y_{N + 2}, \dots, y_{2N} \mid y_1, y_2, \dots, y_N\right)$
-   Others: We have something with a lot of the same symbols $f\left(y_{N + 1}, y_{N + 2}, \dots, y_{2N} \mid \widehat{\boldsymbol{\theta}}, \widehat{\lambda}, y_1, y_2, \dots, y_N\right)$
-   Bayesians: That is quite different from the conditional distribution of future data *irrespective* of the parameters, i.e. $f\left(y_{N + 1}, y_{N + 2}, \dots, y_{2N} \mid y_1, y_2, \dots, y_N\right) = \\\int_\Theta f\left(y_{N + 1}, y_{N + 2}, \dots, y_{2N} \mid \boldsymbol{\theta}\right) f\left(\boldsymbol{\theta} \mid y_1, y_2, \dots, y_N\right)d\boldsymbol{\theta}$
-   Others: We also have confusion matrices
:::

## Estimating the ELPD

-   We typically cannot estimate the ELPD in closed form for the same reason we typically cannot estimate the posterior PDF in closed form

-   But we can estimate the ELPD with posterior draws, plus an additional assumption that when each $y_n$ is omitted $f\left(\boldsymbol{\theta} \mid \mathbf{y}_{-n}, \dots\right) \approx f\left(\boldsymbol{\theta} \mid \mathbf{y}, \dots\right)$. If so, then $f\left(\boldsymbol{\theta} \mid \mathbf{y}_{-n}, \dots\right)$ can be obtained by PSIS of $f\left(\boldsymbol{\theta} \mid \mathbf{y}, \dots\right)$

-   PSISLOOCV estimator of the ELPD involves $\sum_{n = 1}^N \mathbb{E}_{\boldsymbol{\theta} \mid \mathbf{y}_{-n}}\ln f\left(y_{n} \mid \boldsymbol{\theta}\right)$

## Posterior Distribution in a NES Model

```{r}
library(dplyr)
library(rstanarm)
options(mc.cores = parallel::detectCores())
data("nes", package = "rosdata")
nes <- mutate(nes, income = as.factor(income), age = age / 10) |> 
  filter(year == 2000, !is.na(rvote))
nes2000 <- group_by(nes, age, income, white) %>% 
  summarize(R = sum(rvote), D = n() - R, .groups = "drop")
```

```{r, post_logit}
#| cache: true
post_logit <- stan_glm(cbind(R, D) ~ age + I(age^2) + income + white,
                       family = binomial, data = nes2000,
                       prior_intercept = normal(0, 0.2),
                       prior = normal(0, 0.5))
(loo_post_logit <- loo(post_logit, save_psis = TRUE))
```

## Model Comparison with PSISLOOCV

```{r}
post_probit <- update(post_logit, family = binomial(link = "probit"),
                      prior_intercept = normal(0, 0.2 * 1.6),
                      prior = normal(0, 0.5 * 1.6))
loo_post_probit <- loo(post_probit, save_psis = TRUE)
loo_compare(loo_post_logit, loo_post_probit)
```

. . .

This strongly suggests that there is little difference in expected utility between a logit and a probit model, but if you had to choose one, the probit model is slightly preferable in this case. You could take a weighted average of the predictions instead:

```{r}
loo_model_weights(list(logit=loo_post_logit, probit=loo_post_probit))
```

## Confusion Matrices

Supervised learners would prefer something like this (albeit with testing data)

```{r}
betas <- coef(post_logit) # posterior medians as a point estimator
X <- model.matrix(~ age + I(age^2) + income + white, data = nes)
eta <- X %*% betas
table(observed = nes$rvote, model = eta > 0)
```

The lower the count off the diagonal, the better the model.

```{r}
table(observed = nes$rvote, model = 0 < X %*% coef(post_probit))
```

## Posterior Predictive Checking w/ PSIS

```{r}
pp_check(post_probit, plotfun = "loo_pit_overlay", 
         psis_object = loo_post_probit$psis_object)
```

## $R^2$ for GLMs

-   The McElreath reading criticizes the use of (Frequentist) $R^2$, which can only be made larger by including more predictors and fails to capture how well a model predicts future data

-   These problems can be averted by calculating a posterior distribution of $R^2$ values based on how well $y_n$ is predicted by $\boldsymbol{\theta} \mid \mathbf{y}_{-n}$, which is somewhat similar to an "adjusted" $R^2$

```{r}
rbind(worse  = summary(bayes_R2(post_probit)), 
      better = summary(loo_R2(post_probit)))
```

## Comparison with Lasso

```{r}
#| message: false
lasso <- glmnet::glmnet(x = model.matrix(post_logit)[, -1], 
                        y = post_logit$y, family = "binomial")
round(coef(lasso, s = seq(from = 0.1, to = 0.01, length.out = 10)), 2)
```

-   For values of $\lambda$ greater than $0.05$, all estimated coefficients are zero except for that on `white`

-   For smaller values of $\lambda$, more estimates are non-zero

-   What $\lambda$ predicts best in held-out folds (if there were any)?

## How to Proceed?

-   "Frequentist": Proceed as if $\beta_k$ is zero unless you reject the null that it is zero, in which case proceed as if $\beta_k = \widehat{\beta_k}$

    -   This is not Frequentist but an invention of journals

    -   It is an open invitation for $p$-hacking

    -   The distribution of published point estimates is biased

-   Supervised Learning: Proceed as if $\beta_k = \widehat{\beta}_k$, which may be $0$, given the optimal $\lambda$ (based on accuracy in held out folds)

-   Bayesian: Proceed with your (draws from the) posterior distribution of $\beta_k$ , none of which are exactly zero

## Logit Model, No Intercept, 1 Predictor

```{r}
#| echo: false
log_prior <- function(beta_proposal, location = 0, scale = 1 / sqrt(2)) {
  return(-log(2 * scale) - abs( (beta_proposal - location) / scale ))
}
log_sum_exp <- function(a,b) {
  m <- pmax(a,b)
  return( ifelse(a > b, m + log1p(exp(b - m)), 
                        m + log1p(exp(a - m))) )
}
ll <- function(beta_proposal, x, y) {
  stopifnot(is.numeric(beta_proposal), is.numeric(x), is.numeric(y))
  neg_x_beta_proposal <- -outer(x, beta_proposal)
  denominator <- log_sum_exp(0, neg_x_beta_proposal)
  return(colSums(neg_x_beta_proposal[y == 0, , drop = FALSE]) - 
         colSums(denominator))
}
set.seed(12345)
N <- 9
y <- c(rep(1:0, times = 4), 1)
x <- rnorm(N)
LIM <- c(-4, 10)
curve(exp(log_prior(beta)), from = LIM[1], to = LIM[2], xname = "beta", ylab = "On log-scale",
      xlab = expression(beta), log = "y", ylim = c(1e-8, 0.6), n = 1001, las = 1)
curve(exp(ll(beta, x, y)), from = LIM[1], to = LIM[2], xname = "beta", 
      add = TRUE, col = "red", lty = "dashed", log = "y", n = 1001)
kernel <- function(beta, x, y) {
  exp(ll(beta, x, y) + log_prior(beta))
}
denom <- integrate(kernel, x = x, y = y, lower = -Inf, upper = Inf)$value
curve(kernel(beta, x, y) / denom, from = LIM[1], to = LIM[2], xname = "beta", 
      add = TRUE, col = "blue", lty = "dotted", log = "y", n = 1001)
legend("topright", legend = c("Laplace prior", "likelihood", "posterior"), 
       col = c(1,2,4), lty = 1:3, box.lwd = NA)
```
