---
title: "Discrete Probability Distributions"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
---

## Review of Last Time

We defined the probability of knocking down $x \geq 0$ out of $n \geq x$ pins as $\Pr\left(x \mid n\right) = \log_{n + 2}\left(1 + \frac{1}{n + 1 - x}\right)$

```{r}
source("bowling.R") # defines Pr and Omega
```

```{r}
#| code-line-numbers: 1-2|3-4|5-8
R <- 10^7  # practically infinite (to almost match exact calculations)
frames <-  # tibble with the results of R frames of bowling
  tibble(X_1 = sample(Omega, size = R, replace = TRUE, 
                      prob = Pr(Omega))) |> # all R first rolls
  group_by(X_1) |> # then all second rolls, one group at a time
  mutate(X_2 = sample(Omega, size = n(), replace = TRUE, 
                      prob = Pr(Omega, n = 10 - first(X_1)))) |>
  ungroup() # first(X_1) is needed so that n is a scalar in ^^^
```

```{r}
#| code-line-numbers: 1-2|3-4|5-8
joint_Pr <- matrix(0, nrow = length(Omega), ncol = length(Omega),
                   dimnames = list(Omega, Omega))
for (x_1 in Omega) {
  Pr_x_1 <- Pr(x_1, n = 10)
  for (x_2 in 0:(10 - x_1)) {
    joint_Pr[x_1 + 1, x_2 + 1] <- Pr_x_1 * Pr(x_2, n = 10 - x_1)
  } # R indexes starting from 1 (not 0), so have to +1 the indices
}
```

## Probability Via dplyr / siuba Syntax

`|>` pipes a previously created `tibble` to the next function

| Concept             | R Syntax \[after callling **library(dplyr)**\] |
|---------------------|------------------------------------------------|
| $X$                 | `tibble(X = sample(â€¦))` or `|> mutate()`       |
| $\mid X = x$        | `|> filter(X == x) |> ...`                     |
| $\mid X$            | `|> group_by(X) |> ... |> ungroup()`           |
| $\Pr\left(x\right)$ | `|> summarize(prob = mean(X == x))`            |

## Marginal Probabilities via `joint_Pr` {.scrollable .smaller}

```{r, marginal}
#| echo: false
#| message: false
library(knitr)
library(kableExtra)
options("kableExtra.html.bsTable" = TRUE)
options(scipen = 5)
options(knitr.kable.NA = "")
tmp <- as.data.frame(cbind(joint_Pr, " " = -1, 
                           "row-sum" = rowSums(joint_Pr)))
tmp <- rbind(tmp, " " = -1, "col-sum" = colSums(tmp))
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 3), "html", 
                       bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", 
                                      ifelse(tmp[,i] > 
                                               1 - 1e-8 | tmp[,i] < 0, 
                                             "white", "black")))
kable(tmp, format = "html", align = 'c', escape = FALSE,
      table.attr = 'data-quarto-disable-processing="true"') |>
  kable_styling("striped", full_width = FALSE)
```

## Checking the Simulations Again

```{r}
frames |> # vvv is like group_by(frames, X_2) |> summarize(times = n())
  count(X_2, name = "times") |>  # a tibble with 11 rows
  mutate(proportion = times / R, # almost exact but not quite
         probability = colSums(joint_Pr)) # exact
```

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```
. . .

I prefer the notation $\Pr\left(\bcancel{x_1} \bigcap x_2 \mid n\right)$ to refer to the marginal(ized) probability that $X_2 = x_2$, irrespective of $X_1$

## Marginal(ized), Conditional, and Joint

::: incremental
-   To compose a joint (in this case, bivariate) probability, *multiply* a marginal probability by a conditional probability
-   To decompose a joint (in this case, bivariate) probability, *add* the relevant joint probabilities to obtain a marginal probability
-   To obtain a conditional probability, *divide* the joint probability by the marginalized probability of what you're conditioning on: $$\Pr\left(A\bigcap B\right)=\Pr\left(B\right)\times\Pr\left(A \mid B\right) =
    \Pr\left(A\right)\times\Pr\left(B\mid A\right)$$ $$\implies \Pr\left(A \mid B\right)= \frac{\Pr\left(A\right)\times\Pr\left(B\mid A\right)}
    {\Pr\left(B\right)} = 
    \frac{\Pr\left(A \bigcap B\right)}
    {\Pr\left(\bcancel{A} \bigcap B\right)}$$
:::

## Using Bayes' Rule for Bowling

-   How would you compute the probability that $X_1 = 8$ given that $X_2 = 2$ in the same frame of bowling using `frames`?

. . .

```{r}
filter(frames, X_2 == 2) |>
  summarize(cond_prob = mean(X_1 == 8))
```

. . .

-   How would you calculate it exactly using `joint_Pr`?

. . .

```{r}
joint_Pr["8", "2"] / sum(joint_Pr[ , "2"])
```

as compared to a prior (marginalized) probability of knocking down $8$ pins on the first roll, `Pr(8, n = 10)` $= `r Pr(8, n = 10)`$

##  {.smaller}

```{r}
#| echo: false
library(knitr)
library(kableExtra)
options("kableExtra.html.bsTable" = TRUE)
options(scipen = 5)
options(knitr.kable.NA = "")
tmp <- as.data.frame(joint_Pr)
eight <- round(unlist(tmp["8", ]), digits = 4)
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 4), "html", 
                       bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", 
                                      ifelse(i == 3, "black", "blue")))
tmp["8", ] <- cell_spec(eight, "html", bold = eight == 0, 
                        color = ifelse(eight == 0, "red", "green"))
kable(tmp, format = "html", align = 'c', escape = FALSE,
      table.attr = 'data-quarto-disable-processing="true"') |>
  kable_styling("striped", full_width = FALSE)
```

## Bayesian vs Frequentist Probability {.smaller}

-   Bayesians generalize this by taking $A$ to be "whatever you do not know" and $B$ to be "whatever you do know" to manage their beliefs using Bayes' Rule $$\Pr\left(A \mid B\right)= \frac{\Pr\left(A\right)\times\Pr\left(B\mid A\right)}
    {\Pr\left(B\right)} = 
    \frac{\Pr\left(A \bigcap B\right)}
    {\Pr\left(\bcancel{A} \bigcap B\right)}
    $$
-   Utilizing Bayes' Rule is *necessary but not sufficient* to be Bayesian
-   Frequentists accept the validity Bayes' Rule but object to using the language of probability to describe beliefs about unknown propositions and insist that probability is a property of a process that can be defined as a limit $$\Pr\left(A\right) = \lim_{R\uparrow\infty} 
    \frac{\mbox{times that } A \mbox{ occurs in } R \mbox{ independent randomizations}}{R}$$

## Probability an Odd Integer is Prime

::: incremental
-   John Cook [asks](https://www.johndcook.com/blog/2010/10/06/probability-a-number-is-prime/) an instructive question: What is the probability $x$ is prime, where $x$ is like $1 + 10^{100,000,000}$?

-   To Frequentists, $x$ is not a random variable. It is either prime or composite so it makes no sense to say "$x$ is probably $\dots$"

-   To Bayesians, no one knows for sure whether $x$ is prime or composite, but you could chose --- and then update --- a prior probability based on its number of digits, $d$ (when $d$ is large): $\Pr\left(x \mbox{ is prime} \mid d\right) = \frac{1}{d \ln 10} \approx \frac{1}{10^{10} \times 2.3}$
:::

. . .

-   What is the probability that $\beta > 0$ in a regression model?

## [Sanderson](https://www.3blue1brown.com/lessons/bayes-theorem): Scope of Bayes' Rule

![](https://3b1b-posts.us-east-1.linodeobjects.com/content/lessons/2019/bayes-theorem/when-to-use.png){fig-align="center" width="1024" height="393"}

> What's noteworthy is that such a straightforward fact about proportions can become hugely significant for science, AI, and any situation where you want to quantify belief.

## Anti-Bayesian Perspectives

-   Fisher argued "the theory of inverse probability is founded upon an error, and must be wholly rejected" because $H$ is not a random variable so the prior probability, $\Pr\left(H\right)$, the marginalized probability, $\Pr\left(\bcancel{H} \bigcap E\right)$, and the posterior probability $\Pr\left(H \mid E\right)$ are not well-defined quantities or else they are just subjective characteristics of the researcher

-   Frequentism is all about $\Pr\left(E \mid H\right)$, which $\neq \Pr\left(H \mid E\right)$

-   Supervised learning accepts that Bayes Rule is valid, but maintains that probability should not be a prerequisite

## Expectation of a Discrete R.V.

```{r}
round(Pr(Omega), digits = 4) # What's the mode, median, & expectation?
```

::: incremental
-   The mode is the element of $\Omega$ with the highest probability
-   The median is the smallest element of $\Omega$ such that at least half of the cumulative probability is $\leq$ that element
-   Expectation of a discrete random variable $X$ is defined as $$\mathbb{E}\left[X\right] = \sum_{x\in\Omega}\left[x\times\Pr\left(x\right)\right] \equiv \mu$$
-   An expectation is a probability-weighted sum of $\Omega$
:::

## Calculating Expectations in Bowling

-   How would you compute $\mathbb{E}\left[X_1\right]$ using the $R$ `frames`?

. . .

```{r}
summarize(frames, mu_1 = mean(X_1))
```

-   How would you calculate it exactly using `Pr()`?

. . .

```{r}
sum(Omega * Pr(Omega))
```

-   How would you calculate $\mathbb{E}\left[X_2\right]$ exactly using `joint_Pr`?

. . .

```{r}
sum(Omega * colSums(joint_Pr)) # weight with marginal probabilities
```

## Bernoulli Distribution

::: incremental
-   The Bernoulli distribution over $\Omega=\left\{ 0,1\right\}$ depends on a (possibly unknown) probability parameter $\pi \in \left[0,1\right]$

-   By introducing parameters, such as $\pi$, we can make probability distributions more flexible and thus more applicable to a wider variety of situations

-   The probability that $x = 1$ is $\pi$ and the probability that $x = 0$ is $1 - \pi$, which can be written as a Probability Mass Function (PMF): $\Pr\left(x \mid \pi\right)=\pi^{x}\left(1-\pi\right)^{1-x}$

-   What is the expectation of $X$?

-   $\mu = 0 \times \pi^{0}\left(1-\pi\right)^{1-0} + 1 \times \pi^{1}\left(1-\pi\right)^{1-1} = \pi$
:::

## Binomial Distribution

::: incremental
-   A Binomial random variable can be defined as the sum of $n$ independent Bernoulli random variables all with the same $\pi$
-   What is $\Omega$? What is the expectation of $X$?
-   What is an expression for $\Pr\left(x \mid n=3, \pi\right)$? Hint: 8 cases
    -   All succeed, so $\pi^3$ or all fail, so $\left(1 - \pi\right)^3$
    -   1 succeeds and 2 fail $\pi^1 \left(1-\pi\right)^{3 - 1}$ with 3 orderings
    -   2 succeed and 1 fails $\pi^2 \left(1-\pi\right)^{3 - 2}$ with 3 orderings
    -   In general, $\Pr\left(x \mid n,\pi\right)={n \choose x}\pi^{x} \left(1-\pi\right)^{n-x} = \frac{n!}{\left(n - x\right)!x!} \pi^{x} \left(1-\pi\right)^{n-x}$
:::

## Probability of Four Strikes in Bowling

```{r}
frames <- mutate(frames, game = rep(1:(n() / 10), each = 10)) # type
```

-   How would you compute a probability of getting 4 strikes in a game of bowling (consisting of 10 frames) using `frames` ?

. . .

```{r}
group_by(frames, game) |> 
  summarize(four_strikes = sum(X_1 == 10) == 4, .groups = "drop") |> 
  summarize(prob = mean(four_strikes))
```

-   How would you calculate it exactly using the binomial PMF?

. . .

```{r}
c(easy = choose(10, 4) * Pr(10)^4 * (1 - Pr(10))^(10 - 4),
  easier = dbinom(4, size = 10, prob = Pr(10)))
```

## Poisson Distribution for Counts

::: incremental
-   Let $n\uparrow \infty$ and let $\pi \downarrow 0$ such that $\mu = n\pi$ remains fixed. Since $\pi = \frac{\mu}{n}$, what is the limit of the binomial PMF, $\Pr\left(x \mid n, \mu\right)={n \choose x}\left(\mu / n\right)^{x} \left(1-\mu / n\right)^{n-x}$?

    -   ${n \choose x}\pi^{x} = \frac{n!}{x!\left(n - x\right)!} \frac{\mu^x}{n^x} = \frac{n \times \left(n - 1\right) \times \left(n - 2\right) \times \dots \times \left(n - x + 1\right)} {n^x} \frac{\mu^x}{x!}$ $\rightarrow 1 \times \frac{\mu^x}{x!}$
    -   $\left(1-\pi\right)^{n-x} = \left(1-\frac{\mu}{n}\right)^{n-x} = \left(1-\frac{\mu}{n}\right)^n \times \left(1-\frac{\mu}{n}\right)^{-x}$ $\rightarrow e^{-\mu} \times 1$
    -   Thus, the limiting PMF is $\Pr\left(x \mid \mu\right) = \frac{\mu^xe^{-\mu}}{x!}$, which is the PMF of the Poisson distribution over $\Omega = \{0,\mathbb{Z}_+\}$
:::
