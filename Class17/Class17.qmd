---
title: "Introduction to Hierarchical Models"
author: "Ben Goodrich"
format: revealjs
editor: visual
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---

## What Are Hierarchical Models?

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
```

-   In Bayesian terms, a hierarchical model is nothing more than a model where the prior for some parameter depends on another

-   We have already seen several examples of hierarchical structures:

    -   Bowling: $x_2$ depends on $n = 10 - x_1$
    -   Linear models: $\sigma \thicksim \mathcal{E}\left(r\right)$ and $\forall n: \epsilon_n \thicksim \mathcal{N}\left(0,\sigma\right)$

-   It is just another application of the rules of probability: $f\left(\boldsymbol{\theta}\right) = \int f\left(\boldsymbol{\theta}, \boldsymbol{\phi}\right) d\phi_1 \dots d\phi_K = \int f\left(\boldsymbol{\theta} \mid \boldsymbol{\phi}\right) f\left(\boldsymbol{\phi}\right) d\phi_1 \dots d\phi_K$

## Splines

-   "Linear" models are linear functions of the coefficients, but you can always include nonlinear functions of predictors

    -   Interaction terms where three columns of $X$ are generated by two variables and their product
    -   Polynomials where $K$ columns of $X$ are generated by one variable raised to $K$ different powers
    -   Dummy variables that are generated by a factor with $K + 1$ levels

-   Splines use "basis functions", which are much better than polynomials if you want $\eta_n$ to depend on a predictor in a continuous but nonlinear way

## Okun's Law Data

```{r}
source("macroeconomic_data.R")
G <- mgcv::gam(GDO ~ s(x), data = data, fit = FALSE)
X <- G$X
X <- X[ , -1] # these columns each have a mean of zero
colnames(X) <- paste0("X_", 1:ncol(X))
X <- as_tibble(X)
dim(X) # 9 basis functions of change in unemployment
```

Okun never said the relationship was theoretically linear, although he estimated models that assumed so, which is extremely common in the social sciences.

## Model with a Spline

::: columns
::: {.column width="46%"}
Math Notation $\begin{eqnarray*} \forall n: y_n & \equiv & \mu_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(0, \sigma_\epsilon\right) \\ \sigma_\epsilon & \thicksim & \mathcal{E}\left(r_\epsilon\right) \\ \forall n: \mu_{n} & \equiv & \alpha + \\ && \sum_{k = 1}^K \beta_{k} b_k\left(x_{nk}\right) \\ \alpha & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \forall k: \beta_{k} & \thicksim & \mathcal{N}\left(0, \sigma_\beta\right) \\ \sigma_\beta & \thicksim & \mathcal{E}\left(r_\beta\right) \end{eqnarray*}$
:::

::: {.column width="54%"}
::: fragment
Code to Draw

```{r}
R <- 100
priors <- tibble (
  alpha = rnorm(R, 2.5, 0.5),
  sigma_beta = rexp(R, 2),
  sigma_epsilon = rexp(R, 0.2),
  beta = matrix(nrow = R, data = 
    rnorm(R * ncol(X), 
          sd = sigma_beta))
)
predictions <- 
  cross_join(X, priors) |> 
  transmute(mu = alpha + rowSums(
              pick(starts_with("X_")) 
                   * beta),
            epsilon = 
              rnorm(n(), sd =
                    sigma_epsilon),
            y = mu + epsilon)
```
:::
:::
:::

## Posterior Distribution

```{r, Okun}
#| cache: true
#| results: hide
post <- rstanarm::stan_gamm4(GDO ~ s(x), data = data, seed = 12345,
                             prior_intercept = rstanarm::normal(2.5, 0.5),
                             prior_aux = rstanarm::exponential(0.2))
```

```{r}
post
```

## Posterior Plot

```{r}
rstanarm::plot_nonlinear(post)
```

## Restricted Cubic Splines (RCS)

Often it is reasonable to assume a nonlinear function becomes linear through the most extreme data points on either side, which can be accomplished by restricting the derivatives.

```{r}
rstanarm::stan_lm(GDO ~ rms::rcs(x), data = data, seed = 12345,
                  prior_intercept = rstanarm::normal(2.5, 0.5),
                  prior = rstanarm::R2(0.75), refresh = 0, 
                  adapt_delta = 0.999) # to prevent divergences
```

## Gaussian Process (GP) Approximation

-   A Gaussian Process is a prior over smooth functions

-   This requires $N^3$ floating-point operations to evaluate the log-likelihood once, so it can be prohibitive for some datasets that you will encounter in the social sciences

-   The brms package includes a GP approximation that is faster

```{r}
library(brms)
options(mc.cores = parallel::detectCores())
get_prior(GDO ~ gp(x, k = 5, c = 5 / 4), data = data)
```

## GP Approximation Example

```{r, GP}
#| cache: true
#| results: hide
post <- brm(GDO ~ gp(x, k = 5, c = 5 / 4), data = data, prior = 
              prior(normal(2.5, 0.5), class = "Intercept") + 
              prior(exponential(1), class = "sdgp") + 
              prior(exponential(1), class = "sigma"),
            control = list(adapt_delta = 0.9))
```

```{r}
post
```

## Conditional Effects in GPs

```{r}
plot(conditional_effects(post))
```

## Cluster vs. Stratified Sampling

-   For cluster random sampling, you

    -   Sample $J$ large units from their population
    -   Sample $N_j$ small units from the $j$-th large unit

-   For stratified random sampling, you

    -   Divide the population of large units into $J$ mutually exclusive and exhaustive groups that are not RVs
    -   Sample $N_j$ small units from the $j$-th large unit

## Models with Group-Specific Intercepts {.smaller}

-   Let $\alpha$ be the common intercept and $\boldsymbol{\beta}$ be the common coefficients while $a_j$ is the deviation from the common intercept in the $j$-th group. Write a model as: $$y_{ij} = \overbrace{\underbrace{\alpha + \sum_{k = 1}^K \beta_k x_{ik}}_{\mbox{Frequentist }
    \boldsymbol{\mu} \mid \mathbf{x}}+a_j}^{\mbox{Bayesian } \boldsymbol{\mu} \mid \mathbf{x},j} +
    \epsilon_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik}+\underbrace{a_j + 
    \overbrace{\epsilon_{ij}}^{\mbox{Bayesian error}}}_{\mbox{Frequentist error}}$$
-   The same holds in GLMs where $\eta_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik} + a_j$ or $\eta_{ij} = \alpha + \sum_{k = 1}^K \beta_k x_{ik}$ depending on if you are Bayesian or Frequentist

## Models with Group-Specific Slopes {.smaller}

-   Let $\alpha$ be the common intercept and $\boldsymbol{\beta}$ be the common coefficients while $a_j$ is the deviation from the common intercept in the $j$-th group and $\mathbf{b}_j$ is the deviation from the common coefficients. Write the model as: $$y_{ij} = \overbrace{\underbrace{\alpha + \sum_{k = 1}^K \beta_k x_{ik}}_{\mbox{Frequentist }
    \boldsymbol{\mu} \mid \mathbf{x}} + a_j + \sum_{k = 1}^K b_{jk} x_{ik}}^{\mbox{Bayesian } \boldsymbol{\mu} \mid \mathbf{x},j} + \epsilon_{ij} = \\ \alpha + \sum_{k = 1}^K \beta_k x_{ik}+\underbrace{a_j + \sum_{k = 1}^K b_{jk} x_{ik} + \overbrace{\epsilon_{ij}}^{\mbox{Bayesian error}}}_{\mbox{Frequentist error}}$$
-   And similarly for GLMs that utilize an inverse link function

## [Frequentist Estimation of MLMs](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)

-   Frequentists assume that $a_j$ and $b_j$ deviate from the common parameters according to a (multivariate) normal distribution, whose (co)variances are common parameters
-   To Frequentists, $a_j$ and $b_j$ are not parameters because parameters must remain fixed in repeated sampling of observations from some population
-   Since $a_j$ and $b_j$ are not parameters, they can't be "estimated" only "predicted"
-   Since $a_j$ and $b_j$ aren't estimated, they must be integrated out of the likelihood function, leaving an integrated likelihood function of the common parameters

## Bivariate Normal over $S = \mathbb{R}^2$

$f\left(x,y\mid \mu_X,\mu_Y,\sigma_X,\sigma_Y,\rho\right) =\\ \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}e^{-\frac{1}{2\left(1-\rho^2\right)} \left(\left(\frac{x - \mu_X}{\sigma_X}\right)^2 + \left(\frac{y - \mu_Y}{\sigma_Y}\right)^2 - 2\rho\frac{x - \mu_X}{\sigma_X}\frac{y - \mu_Y}{\sigma_Y}\right)} = \\ \frac{1}{\sigma_X\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x - \mu_X}{\sigma_X}\right)^2} \times \frac{1}{\color{blue}{\sigma_Y\sqrt{1-\rho^2}}\sqrt{2\pi}}e^{-\frac{1}{2} \left(\frac{y - \left(\color{red}{\mu_Y + \frac{\sigma_Y}{\sigma_X}\rho\left(x-\mu_x\right)}\right)} {\color{blue}{\sigma_Y\sqrt{1-\rho^2}}}\right)^2}$ where the first term is a marginal normal PDF for $X$ and the second is a conditional normal PDF for $Y \mid X = x$ with new parameters $\color{red}{\mu = \mu_Y + \frac{\sigma_Y}{\sigma_X}\rho\left(x-\mu_X\right)}$ & $\color{blue}{\sigma = \sigma_Y\sqrt{1-\rho^2}}$

## Multilevel Data-Generating Processes {.smaller}

::: columns
::: {.column width="50%"}
Bayesian $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(0, \sigma_\epsilon\right) \\ \sigma_\epsilon & \thicksim & \mathcal{E}\left(r_\epsilon\right) \\ \forall n: \eta_{n} & \equiv & \alpha + a_j + \left(\beta + b_j\right) x_{n} \\ \alpha & \thicksim & \mathcal{N}\left(m_0, s_0\right) \\ \beta & \thicksim & \mathcal{N}\left(m_1, s_1\right) \\ \forall j: b_j & \thicksim & \mathcal{N}\left(\frac{\sigma_b}{\sigma_a} \rho a_j, \sqrt{1 - \rho^2} \sigma_b\right) \\ \forall j: a_j & \thicksim & \mathcal{N}\left(0, \sigma_a\right) \\ \sigma_a, \sigma_b & \thicksim & \mathcal{E}\left(r\right) \\ \rho & \thicksim & \mathcal{U}\left(-1,1\right) \end{eqnarray*}$
:::

::: {.column width="50%"}
::: fragment
Frequentist $\begin{eqnarray*} \forall n: y_n & \equiv & \eta_n + \epsilon_{n} \\ \forall n: \epsilon_{n} & \thicksim & \mathcal{N}\left(a_j + b_j x_n, \sigma_\epsilon\right) \\ \sigma_\epsilon & \text{is} & \text{given} \\ \forall n: \eta_{n} & \equiv & \alpha + \beta + x_{n} \\ \alpha & \text{is} & \text{given} \\ \beta & \text{is} & \text{given} \\ \forall j: b_j & \thicksim & \mathcal{N}\left(\frac{\sigma_b}{\sigma_a} \rho a_j, \sqrt{1 - \rho^2} \sigma_b\right) \\ \forall j: a_j & \thicksim & \mathcal{N}\left(0, \sigma_a\right) \\ \sigma_a, \sigma_b & \text{are} & \text{given} \\ \rho & \text{is} & \text{given} \end{eqnarray*}$
:::
:::
:::

## Table 2 from the lme4 [Vignette](https://www.jstatsoft.org/article/view/v067i01/0)

![](lme4Syntax.png)

